<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Contrastive Loss Functions</title>
    <!-- Include MathJax for rendering mathematical equations -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <!-- Include a CSS library for syntax highlighting -->
    <link rel="stylesheet"
          href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 40px;
            line-height: 1.6;
        }
        h1, h2, h3 {
            color: #2c3e50;
        }
        pre {
            background: #f4f4f4;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
        }
        code {
            font-family: 'Courier New', Courier, monospace;
        }
    </style>
</head>
<body>

    <h1>Contrastive Loss Functions in Machine Learning</h1>
    <p>Contrastive loss functions are fundamental in self-supervised and unsupervised learning paradigms. They enable models to learn meaningful representations by contrasting positive (similar) pairs against negative (dissimilar) pairs.</p>

    <h2>1. Contrastive Loss (Pair Function)</h2>
    <p>The contrastive loss function operates on pairs of samples, encouraging the model to bring similar pairs closer in the embedding space while pushing dissimilar pairs apart.</p>

    <h3>Mathematical Formulation</h3>
    <p>The contrastive loss for a pair of samples \( (x_1, x_2) \) with label \( y \) is defined as:</p>
    <p>
        \[
        \mathcal{L} = y \cdot D^2 + (1 - y) \cdot \max(0, m - D)^2
        \]
    </p>
    <p>Where:</p>
    <ul>
        <li>\( D \) is the Euclidean distance between the embeddings of \( x_1 \) and \( x_2 \).</li>
        <li>\( y = 1 \) if \( x_1 \) and \( x_2 \) are similar (positive pair), else \( y = 0 \).</li>
        <li>\( m \) is the margin that defines the minimum distance for dissimilar pairs.</li>
    </ul>

    <h3>Implementation Example (PyTorch)</h3>
    <pre><code class="python">
import torch
import torch.nn as nn

class ContrastiveLoss(nn.Module):
    def __init__(self, margin=1.0):
        super(ContrastiveLoss, self).__init__()
        self.margin = margin

    def forward(self, output1, output2, label):
        euclidean_distance = torch.nn.functional.pairwise_distance(output1, output2)
        loss = torch.mean(
            label * torch.pow(euclidean_distance, 2) +
            (1 - label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2)
        )
        return loss
    </code></pre>

    <h2>2. InfoNCE Loss</h2>
    <p>InfoNCE is a type of contrastive loss that leverages multiple negative samples within a batch to improve representation learning. It's widely used in frameworks like SimCLR.</p>

    <h3>Mathematical Formulation</h3>
    <p>The InfoNCE loss for a positive pair \( (i, j) \) is defined as:</p>
    <p>
        \[
        \mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}
        \]
    </p>
    <p>Where:</p>
    <ul>
        <li>\( z_i \) and \( z_j \) are the embeddings of the positive pair.</li>
        <li>\( \text{sim}(a, b) \) is a similarity function, typically cosine similarity.</li>
        <li>\( \tau \) is a temperature parameter that scales the logits.</li>
        <li>\( N \) is the batch size, and \( 2N \) accounts for all positive and negative pairs in the batch.</li>
        <li>\( \mathbb{1}_{k \neq i} \) is an indicator function that excludes the positive pair from the denominator.</li>
    </ul>

    <h3>Implementation Example (PyTorch)</h3>
    <pre><code class="python">
import torch
import torch.nn as nn
import torch.nn.functional as F

def info_nce_loss(features, batch_size, temperature=0.5):
    # Normalize the feature vectors
    features = F.normalize(features, dim=1)
    
    # Compute similarity matrix
    similarity_matrix = torch.matmul(features, features.T)
    
    # Create labels for positive pairs
    labels = torch.arange(batch_size).to(features.device)
    labels = torch.cat([labels, labels], dim=0)
    
    # Adjust similarity matrix
    mask = torch.eye(batch_size * 2, dtype=torch.bool).to(features.device)
    similarity_matrix = similarity_matrix / temperature
    similarity_matrix = similarity_matrix.masked_fill(mask, -1e9)
    
    # Compute logits and labels
    logits = similarity_matrix
    labels = torch.arange(batch_size).to(features.device)
    
    loss = F.cross_entropy(logits, labels)
    return loss
    </code></pre>

    <h2>3. NT-Xent Loss (Normalized Temperature-scaled Cross Entropy Loss)</h2>
    <p>NT-Xent is a specific formulation of the InfoNCE loss used in the SimCLR framework. It emphasizes the normalization of the loss over positive and negative pairs.</p>

    <h3>Mathematical Formulation</h3>
    <p>The NT-Xent loss for a positive pair \( (i, j) \) is defined as:</p>
    <p>
        \[
        \mathcal{L}_{\text{NT-Xent}} = -\log \frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N} \mathbb{1}_{k \neq i} \exp(\text{sim}(z_i, z_k)/\tau)}
        \]
    </p>
    <p>It's essentially the same as InfoNCE but tailored for the SimCLR architecture by incorporating batch-wise negatives and ensuring symmetry in the loss computation.</p>

    <h3>Implementation Example (PyTorch)</h3>
    <pre><code class="python">
import torch
import torch.nn as nn
import torch.nn.functional as F

class NTXentLoss(nn.Module):
    def __init__(self, batch_size, temperature=0.5, device='cuda'):
        super(NTXentLoss, self).__init__()
        self.batch_size = batch_size
        self.temperature = temperature
        self.device = device
        self.mask = self._create_mask(batch_size)
        self.criterion = nn.CrossEntropyLoss(reduction="sum")
        self.similarity_function = nn.CosineSimilarity(dim=2)
        
    def _create_mask(self, batch_size):
        N = 2 * batch_size
        mask = torch.ones((N, N), dtype=bool)
        mask = mask.fill_diagonal_(False)
        for i in range(batch_size):
            mask[i, batch_size + i] = False
            mask[batch_size + i, i] = False
        return mask

    def forward(self, zi, zj):
        N = 2 * self.batch_size
        zi = F.normalize(zi, dim=1)
        zj = F.normalize(zj, dim=1)
        representations = torch.cat([zi, zj], dim=0)
        similarity_matrix = self.similarity_function(
            representations.unsqueeze(1), representations.unsqueeze(0)
        ) / self.temperature
        similarity_matrix = similarity_matrix.masked_select(self.mask.to(self.device)).view(N, -1)
        
        positives = torch.cat([torch.diag(similarity_matrix, self.batch_size),
                               torch.diag(similarity_matrix, -self.batch_size)], dim=0).view(N, 1)
        labels = torch.zeros(N).long().to(self.device)
        logits = torch.cat([positives, similarity_matrix], dim=1)
        loss = self.criterion(logits, labels)
        loss /= N
        return loss
    </code></pre>

    <h2>Conclusion</h2>
    <p>Contrastive loss functions like Contrastive Loss, InfoNCE, and NT-Xent play a pivotal role in learning effective representations without relying on labeled data. They achieve this by leveraging the relationships between positive and negative pairs, enabling models to capture the underlying structure of the data.</p>

</body>
</html>
