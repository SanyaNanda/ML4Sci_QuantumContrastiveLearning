{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b2d1985-64cb-4351-95b4-d7afbef0f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, losses, optimizers, initializers\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e99280c-7423-456e-a35b-a2c92e43a3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    # loading the data as train and test\n",
    "    data = np.load('../../data/electron-photon-pairs.npz', allow_pickle=True)\n",
    "    pairs_train = data[\"pairs_train\"]\n",
    "    labels_train = data[\"labels_train\"]\n",
    "    pairs_test = data[\"pairs_test\"]\n",
    "    labels_test = data[\"labels_test\"]\n",
    "    return pairs_train, labels_train, pairs_test, labels_test\n",
    "\n",
    "def create_data_augmentation_layer():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "        tf.keras.layers.RandomRotation(0.2),\n",
    "        tf.keras.layers.RandomZoom(0.2),\n",
    "        tf.keras.layers.RandomContrast(0.2),\n",
    "    ])\n",
    "\n",
    "# Base model from classical nb, adding qlayer in between\n",
    "def create_base_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Input(shape=input_shape))\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer=initializers.HeNormal())) # Conv layer 1\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.01))) # Conv layer 2\n",
    "    model.add(layers.Dropout((0.5)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='leaky_relu',kernel_regularizer=tf.keras.regularizers.l2(0.01)))\n",
    "    return model\n",
    "    \n",
    "\n",
    "class SiameseModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.base_model = create_base_model(input_shape)\n",
    "        self.data_augmentation = create_data_augmentation_layer()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        img1, img2 = inputs\n",
    "        img1 = self.data_augmentation(img1)\n",
    "        img2 = self.data_augmentation(img2)\n",
    "        feat1 = self.base_model(img1)\n",
    "        feat2 = self.base_model(img2)\n",
    "        distance = layers.Lambda(lambda embeddings: tf.sqrt(tf.reduce_sum(tf.square(embeddings[0] - embeddings[1]), axis=-1)), output_shape=(1,)\n",
    "                            )([feat1, feat2])\n",
    "    \n",
    "        model = models.Model([img1, img2], distance)\n",
    "        return model\n",
    "\n",
    "def info_nce_loss(temperature=0.1):\n",
    "    def loss(features1,features2):\n",
    "        batch_size = tf.shape(features1)[0]\n",
    "        labels = tf.range(batch_size)\n",
    "            \n",
    "        features1_norm = tf.math.l2_normalize(features1, axis=1, epsilon=tf.cast(1e-12, tf.float32))\n",
    "        features2_norm = tf.math.l2_normalize(features2, axis=1, epsilon=tf.cast(1e-12, tf.float32))\n",
    "    \n",
    "            \n",
    "        logits = tf.matmul(features1_norm, features2_norm, transpose_b=True) / temperature\n",
    "         \n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f83263d8-68b4-45a2-a2f0-de35ee434177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Sanya Nanda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sanya Nanda\\AppData\\Local\\Temp\\ipykernel_138732\\3549115620.py\", line 52, in loss  *\n        features1_norm = tf.math.l2_normalize(features1, axis=1, epsilon=tf.cast(1e-12, tf.float32))\n\n    TypeError: Input 'y' of 'Maximum' Op has type float32 that does not match type int32 of argument 'x'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m siamese_model \u001b[38;5;241m=\u001b[39m SiameseModel(input_shape)\n\u001b[0;32m      8\u001b[0m siamese_model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39minfo_nce_loss(), optimizer\u001b[38;5;241m=\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m))\n\u001b[1;32m---> 10\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43msiamese_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_images\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[cp_callback]\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# # Compile the model\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# After training, you can use the base_model for feature extraction or fine-tuning\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\SANYAN~1\\AppData\\Local\\Temp\\__autograph_generated_filee66xmmw3.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Users\\SANYAN~1\\AppData\\Local\\Temp\\__autograph_generated_file7qadfhjv.py:13\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__loss\u001b[1;34m(features1, features2)\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mshape, (ag__\u001b[38;5;241m.\u001b[39mld(features1),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     12\u001b[0m labels \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrange, (ag__\u001b[38;5;241m.\u001b[39mld(batch_size),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m---> 13\u001b[0m features1_norm \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2_normalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1e-12\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m features2_norm \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39ml2_normalize, (ag__\u001b[38;5;241m.\u001b[39mld(features2),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, epsilon\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mcast, (\u001b[38;5;241m1e-12\u001b[39m, ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mfloat32), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)), fscope)\n\u001b[0;32m     15\u001b[0m logits \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mmatmul, (ag__\u001b[38;5;241m.\u001b[39mld(features1_norm), ag__\u001b[38;5;241m.\u001b[39mld(features2_norm)), \u001b[38;5;28mdict\u001b[39m(transpose_b\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), fscope) \u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(temperature)\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Sanya Nanda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Sanya Nanda\\AppData\\Local\\Temp\\ipykernel_138732\\3549115620.py\", line 52, in loss  *\n        features1_norm = tf.math.l2_normalize(features1, axis=1, epsilon=tf.cast(1e-12, tf.float32))\n\n    TypeError: Input 'y' of 'Maximum' Op has type float32 that does not match type int32 of argument 'x'.\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess your data\n",
    "train_images, train_labels, test_images, test_labels = load_and_preprocess_data()\n",
    "\n",
    "# Define the model\n",
    "input_shape = train_images.shape[2:]\n",
    "siamese_model = SiameseModel(input_shape)\n",
    "\n",
    "siamese_model.compile(loss=info_nce_loss(), optimizer=optimizers.Adam(learning_rate=1e-3))\n",
    "\n",
    "history = siamese_model.fit([train_images[:, 0], train_images[:, 1]], train_labels,\n",
    "    validation_data=([test_images[:, 0], test_images[:, 1]], test_labels),\n",
    "    epochs=10,\n",
    "    batch_size=5000,\n",
    "    # callbacks=[cp_callback]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# # Compile the model\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# @tf.function\n",
    "# def train_step(images, labels):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         features1, features2 = siamese_model((images[:0], images[:1]), training=True)\n",
    "#         loss = info_nce_loss(features1, features2)\n",
    "    \n",
    "#     gradients = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, siamese_model.trainable_variables))\n",
    "    \n",
    "#     return loss\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 10\n",
    "# batch_size = 32\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "#     for i in range(0, len(train_images), batch_size):\n",
    "#         batch_images = train_images[i:i+batch_size]\n",
    "#         batch_labels = train_labels[i:i+batch_size]\n",
    "        \n",
    "#         loss = train_step(batch_images, batch_labels)\n",
    "        \n",
    "#         if i % 100 == 0:\n",
    "#             print(f\"  Step {i//batch_size}: loss = {loss:.4f}\")\n",
    "\n",
    "# After training, you can use the base_model for feature extraction or fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3afeb6e-e1cb-4920-b89f-4ae2fd870613",
   "metadata": {},
   "source": [
    "$\\mathcal{L}{\\text{InfoNCE}} = -\\log \\frac{\\exp(sim(f_i, f_j^+) / \\tau)}{\\sum{k=1}^N \\exp(sim(f_i, f_k) / \\tau)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f5bdba-4655-4810-a7b8-0e62b2181feb",
   "metadata": {},
   "source": [
    "$F(\\rho_1, \\rho_2) = |\\langle \\psi_1 | \\psi_2 \\rangle|^2$\n",
    "where $|\\psi_1\\rangle$ and $|\\psi_2\\rangle$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce7ae8-b3a0-4332-a787-c1a5b577a1d5",
   "metadata": {},
   "source": [
    "$\\mathcal{L}{\\text{total}} = \\alpha \\mathcal{L}{\\text{InfoNCE}} + (1 - \\alpha) (1 - F(\\rho_1, \\rho_2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f70682-f60f-48a6-b908-7accbc187ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
